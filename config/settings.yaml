#config/settings.txt
version: 1
runtime:
  log_level: INFO
  reconnect_backoff_s: [1, 2, 5, 10, 20]
  heartbeat_interval_s: 20
symbols:
  - R_10

# Configuration for main launcher and Phase 2 training orchestration.
# Edit these values per-run or copy into training/experiments/run_{id}/config.json for reproducibility.


paths:
  base_dir: "data"                              # root where data/<symbol>/features, parity, live_agg live
  selected_shards: "training/selected_shards.jsonl"
  candles_ndjson: "training/candles_1m.ndjson"
  scalers: "training/scalers_live_only.json"
  experiments_root: "training/experiments"      # per-run experiment directories will be created here

data:
  min_cov_for_training: 0.8                     # minimal manifest.bidask_coverage to accept shard for training
  min_shard_rows: 1000                          # minimal rows per shard
  split_method: "proportion"                    # "proportion" or "epochs"
  split_proportions: [0.8, 0.1, 0.1]            # train/val/test proportions
  bidask_thresh: 0.8                            # threshold used for scalers live-backed split

model:
  model_type: "mlp"                             # "mlp" or "transformer" (affects train.py flags only)
  hidden_dims: [256, 128]
  dropout: 0.1
  use_mdn: false
  n_mixtures: 5
  use_cls_head: true
  regime_cardinality: 10

training:
  batch_size: 256
  lr: 0.001
  epochs: 100
  early_stop_patience: 10
  seed: 42
  cls_weight: 0.5

# Orchestration
run_phase2: true                                # set false to skip Phase2 and only run Phase1
phase2_start_delay_seconds: 1000                  # delay in seconds before Phase2 starts after launcher is up
